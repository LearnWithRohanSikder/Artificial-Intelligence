<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (18) on Thu Feb 09 18:28:27 GMT 2023 -->
<title>BackpropagationTrainer</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="dc.created" content="2023-02-09">
<meta name="description" content="declaration: module: atu.aicme4j, package: jhealy.aicme4j.net, class: BackpropagationTrainer">
<meta name="generator" content="javadoc/ClassWriterImpl">
<link rel="stylesheet" type="text/css" href="../../../../stylesheet.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../../script-dir/jquery-ui.min.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../../jquery-ui.overrides.css" title="Style">
<script type="text/javascript" src="../../../../script.js"></script>
<script type="text/javascript" src="../../../../script-dir/jquery-3.5.1.min.js"></script>
<script type="text/javascript" src="../../../../script-dir/jquery-ui.min.js"></script>
</head>
<body class="class-declaration-page">
<script type="text/javascript">var evenRowColor = "even-row-color";
var oddRowColor = "odd-row-color";
var tableTab = "table-tab";
var activeTableTab = "active-table-tab";
var pathtoroot = "../../../../";
loadScripts(document, 'script');</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<div class="flex-box">
<header role="banner" class="flex-header">
<nav role="navigation">
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="top-nav" id="navbar-top"><button id="navbar-toggle-button" aria-controls="navbar-top" aria-expanded="false" aria-label="Toggle navigation links"><span class="nav-bar-toggle-icon"></span><span class="nav-bar-toggle-icon"></span><span class="nav-bar-toggle-icon"></span></button>
<div class="skip-nav"><a href="#skip-navbar-top" title="Skip navigation links">Skip navigation links</a></div>
<ul id="navbar-top-firstrow" class="nav-list" title="Navigation">
<li><a href="../../../module-summary.html">Module</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="nav-bar-cell1-rev">Class</li>
<li><a href="class-use/BackpropagationTrainer.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../index-files/index-1.html">Index</a></li>
<li><a href="../../../../help-doc.html#class">Help</a></li>
</ul>
<ul class="sub-nav-list-small">
<li>
<p>Summary:</p>
<ul>
<li>Nested</li>
<li>Field</li>
<li>Constr</li>
<li><a href="#method-summary">Method</a></li>
</ul>
</li>
<li>
<p>Detail:</p>
<ul>
<li>Field</li>
<li>Constr</li>
<li><a href="#method-detail">Method</a></li>
</ul>
</li>
</ul>
</div>
<div class="sub-nav">
<div id="navbar-sub-list">
<ul class="sub-nav-list">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method-summary">Method</a></li>
</ul>
<ul class="sub-nav-list">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method-detail">Method</a></li>
</ul>
</div>
<div class="nav-list-search"><label for="search-input">SEARCH:</label>
<input type="text" id="search-input" disabled placeholder="Search">
<input type="reset" id="reset-button" disabled value="reset">
</div>
</div>
<!-- ========= END OF TOP NAVBAR ========= -->
<span class="skip-nav" id="skip-navbar-top"></span></nav>
</header>
<div class="flex-content">
<main role="main">
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="sub-title"><span class="module-label-in-type">Module</span>&nbsp;<a href="../../../module-summary.html">atu.aicme4j</a></div>
<div class="sub-title"><span class="package-label-in-type">Package</span>&nbsp;<a href="package-summary.html">jhealy.aicme4j.net</a></div>
<h1 title="Class BackpropagationTrainer" class="title">Class BackpropagationTrainer</h1>
</div>
<div class="inheritance" title="Inheritance Tree"><a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html" title="class or interface in java.lang" class="external-link">java.lang.Object</a>
<div class="inheritance">jhealy.aicme4j.net.BackpropagationTrainer</div>
</div>
<section class="class-description" id="class-description">
<hr>
<div class="type-signature"><span class="modifiers">public class </span><span class="element-name type-name-label">BackpropagationTrainer</span>
<span class="extends-implements">extends <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html" title="class or interface in java.lang" class="external-link">Object</a></span></div>
<div class="block">An implementation of the algorithm for training a multi-layer feed-forward 
 neural network described by Bryson and Ho (1969). A backpropagation network 
 typically has 3 - 4 fully connected layers. Each hidden layer increases the 
 computational burden of training exponentially. A single hidden layer is
 enough for many tasks.
 
 <p>The backpropagation algorithm feeds the input vector to the neural network 
 forward to the output layer, computes the error for each output node and then 
 propagates the error back through each layer of the network for weight adjustment. 
 The main steps involved are shown in the following algorithm: 
 
 <pre>
 Initialise weights to random numbers in range
 Keep doing epochs
   For each example in the training set Do
      Feedforward {X1, X2,â€¦,Xn} to each node n
      Y = output(n)
      error = (Yd - Y) at each output neuron 
      Backpropagate error to calculate deltas to weights
      Update all weights
   End For
 Until training set error stops improving
 </pre> 
 
 <p>The algorithm uses gradient descent to minimise the training error by finding
 the lowest point on the error surface as shown below. If the slope is very steep,
 then a large change will be made to the weights. As the algorithm progressively 
 approaches the optimal weights for each layer, the error rate will reduce and
 smaller changes will be made to the weights.
 
 <p><img alt="Neural Network" src="./doc-files/gradient-descent.svg"/>
 
 <p>The slope of the gradient is computed using the derivative of the activation 
 function for each layer. The derivative of a function <em>y = f(x)</em> is a measure 
 of the rate at which the value <em>y</em> of the function changes with respect to 
 <em>x</em>, i.e. its slope. In this implementation, the sum of the squared errors 
 is used to determine if the actual v/s expected outputs have converged.
 
 <p>The weights between each layer in the network are updated using the <b>Delta
 Rule:</b> <i>&Delta;w<sub>ni</sub> = &alpha; * (y<sub>dn</sub> - y<sub>n</sub>) 
 * f'(h<sub>n</sub>) * x<sub>i</sub></i> where <em>n</em> is a node or neuron, 
 <em>i</em> is <em>n</em>'s <em>i</em>th weight, &alpha; is the learning rate,
 <em>f'</em> is the derivative of the activation function used at the layer, 
 <em>y<sub>dn</sub></em> is the desired output, <em>y<sub>n</sub></em> is the actual 
 output, <em>h<sub>n</sub></em>is the weighted sum of <em>n</em>'s inputs and <em>x<sub>i</sub></em> 
 is the <em>i</em>th input. 
 
 <p>Changing the value of the <em>learning rate</em> (&alpha;) is one of the most effective ways of 
 accelerating convergence. A small value for &alpha; will result in smaller changes to the 
 weights, a slower rate of learning and a smooth learning curve. Using a large &alpha; value 
 will accelerate learning, but may induce instability to the neural network and cause 
 oscillations (swinging between positive and negative errors) and abrupt changes to outputs.
 A small value for &alpha; can be compared to taking a 4 year computing degree at a university: it
 will take you a long time to finish, but you will be well-trained and well-prepared when you do. A
 high value for &alpha; is like taking a 6 or 12 month accelerated course in computing. You will
 have to learn very quickly, but the depth of learning will not be the same as a 4-year degree 
 course and you might find that you've mastered very little at the end of the training.
 
 A common technique to accelerate learning is to add an extra <em>momentum</em> parameter (&beta;) 
 to the delta rule to accelerate gradient descent, where <em>0 &leq; &beta; &lt; 1</em>. A &beta; 
 value of 0.95 is often used for momentum.</div>
<dl class="notes">
<dt>Since:</dt>
<dd>Aicme4j 1.0</dd>
<dt>Author:</dt>
<dd>Dr. John Healy, ATU.</dd>
</dl>
</section>
<section class="summary">
<ul class="summary-list">
<!-- ========== METHOD SUMMARY =========== -->
<li>
<section class="method-summary" id="method-summary">
<h2>Method Summary</h2>
<div id="method-summary-table">
<div class="table-tabs" role="tablist" aria-orientation="horizontal"><button id="method-summary-table-tab0" role="tab" aria-selected="true" aria-controls="method-summary-table.tabpanel" tabindex="0" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table', 3)" class="active-table-tab">All Methods</button><button id="method-summary-table-tab2" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab2', 3)" class="table-tab">Instance Methods</button><button id="method-summary-table-tab4" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab4', 3)" class="table-tab">Concrete Methods</button></div>
<div id="method-summary-table.tabpanel" role="tabpanel">
<div class="summary-table three-column-summary" aria-labelledby="method-summary-table-tab0">
<div class="table-header col-first">Modifier and Type</div>
<div class="table-header col-second">Method</div>
<div class="table-header col-last">Description</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab4"><code><a href="TrainingStatistics.html" title="class in jhealy.aicme4j.net">TrainingStatistics</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab4"><code><a href="#train(double%5B%5D%5B%5D,double%5B%5D%5B%5D,double,double,int,double,jhealy.aicme4j.net.Loss,boolean)" class="member-name-link">train</a><wbr>(double[][]&nbsp;data,
 double[][]&nbsp;desired,
 double&nbsp;alpha,
 double&nbsp;momentum,
 int&nbsp;epochs,
 double&nbsp;maxError,
 <a href="Loss.html" title="enum class in jhealy.aicme4j.net">Loss</a>&nbsp;loss,
 boolean&nbsp;softmax)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab4">
<div class="block">Trains a valid instance of <a href="NeuralNetwork.html" title="class in jhealy.aicme4j.net"><code>NeuralNetwork</code></a> using the backpropagation
 algorithm.</div>
</div>
</div>
</div>
</div>
<div class="inherited-list">
<h3 id="methods-inherited-from-class-java.lang.Object">Methods inherited from class&nbsp;java.lang.<a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html" title="class or interface in java.lang" class="external-link">Object</a></h3>
<code><a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#clone()" title="class or interface in java.lang" class="external-link">clone</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#equals(java.lang.Object)" title="class or interface in java.lang" class="external-link">equals</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#finalize()" title="class or interface in java.lang" class="external-link">finalize</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#getClass()" title="class or interface in java.lang" class="external-link">getClass</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#hashCode()" title="class or interface in java.lang" class="external-link">hashCode</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#notify()" title="class or interface in java.lang" class="external-link">notify</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#notifyAll()" title="class or interface in java.lang" class="external-link">notifyAll</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#toString()" title="class or interface in java.lang" class="external-link">toString</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#wait()" title="class or interface in java.lang" class="external-link">wait</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#wait(long)" title="class or interface in java.lang" class="external-link">wait</a>, <a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Object.html#wait(long,int)" title="class or interface in java.lang" class="external-link">wait</a></code></div>
</section>
</li>
</ul>
</section>
<section class="details">
<ul class="details-list">
<!-- ============ METHOD DETAIL ========== -->
<li>
<section class="method-details" id="method-detail">
<h2>Method Details</h2>
<ul class="member-list">
<li>
<section class="detail" id="train(double[][],double[][],double,double,int,double,jhealy.aicme4j.net.Loss,boolean)">
<h3>train</h3>
<div class="member-signature"><span class="modifiers">public</span>&nbsp;<span class="return-type"><a href="TrainingStatistics.html" title="class in jhealy.aicme4j.net">TrainingStatistics</a></span>&nbsp;<span class="element-name">train</span><wbr><span class="parameters">(double[][]&nbsp;data,
 double[][]&nbsp;desired,
 double&nbsp;alpha,
 double&nbsp;momentum,
 int&nbsp;epochs,
 double&nbsp;maxError,
 <a href="Loss.html" title="enum class in jhealy.aicme4j.net">Loss</a>&nbsp;loss,
 boolean&nbsp;softmax)</span>
                         throws <span class="exceptions"><a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Exception.html" title="class or interface in java.lang" class="external-link">Exception</a></span></div>
<div class="block">Trains a valid instance of <a href="NeuralNetwork.html" title="class in jhealy.aicme4j.net"><code>NeuralNetwork</code></a> using the backpropagation
 algorithm.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>data</code> - the data set to use for the supervised training</dd>
<dd><code>desired</code> - the expected output for each row in the training set</dd>
<dd><code>alpha</code> - the learning rate of the training algorithm</dd>
<dd><code>momentum</code> - a hyperparameter to help control the rate of gradient descent</dd>
<dd><code>epochs</code> - the maximum number of epochs used in the training</dd>
<dd><code>maxError</code> - maxError the maximum error of the training</dd>
<dd><code>loss</code> - the loss function to use in the training</dd>
<dd><code>softmax</code> - transform the output layer into a probability distribution</dd>
<dt>Returns:</dt>
<dd>a trained instance of NetworkBuilder</dd>
<dt>Throws:</dt>
<dd><code><a href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/lang/Exception.html" title="class or interface in java.lang" class="external-link">Exception</a></code> - if the network topology has not been correctly configured or 
 the training algorithm encountered a problem like a vanishing or exploding 
 gradient.</dd>
</dl>
</section>
</li>
</ul>
</section>
</li>
</ul>
</section>
<!-- ========= END OF CLASS DATA ========= -->
</main>
</div>
</div>
</body>
</html>
